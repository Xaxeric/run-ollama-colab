{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run ollama in a single Colab Session\n",
        "\n",
        "To enable GPU in this notebook, select Runtime -> Change runtime type in the Menu bar. Under Hardware Accelerator, select GPU.\n",
        "\n",
        "Then, scroll to the Configuration [cell](#scrollTo=8WIVDY-V-kVw&line=1&uniqifier=1) and update it with your ngrok authentication token.\n",
        "\n",
        "To run, select Runtime -> Run all. Go to this [cell](#scrollTo=GZiFjH5QZFrd&uniqifier=1) and read the instructions on how to update your `.env` file."
      ],
      "metadata": {
        "id": "4cFX5ElsYbPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU"
      ],
      "metadata": {
        "id": "e_2rOjTRCuGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "2uEbLKtCCtcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Deps"
      ],
      "metadata": {
        "id": "9_RqReHfCwPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update -y -qq\n",
        "!apt install -y -qq curl lshw libcairo2-dev pkg-config python3-dev\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!pip install flask -q\n",
        "!pip install pyngrok -q\n",
        "!pip install requests -q\n",
        "!pip install flask-cors -q\n",
        "\n",
        "import subprocess\n",
        "sub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n",
        "!ollama pull zephyr"
      ],
      "metadata": {
        "id": "vKClg4iV3tnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "\n",
        "Please set the NGROK auth token to access the tunnel."
      ],
      "metadata": {
        "id": "kY-BMho4soa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = '' #@param {type:'string'}\n",
        "OLLAMA_URL = 'http://127.0.0.1:11434' #@param {type:'string'}"
      ],
      "metadata": {
        "id": "8WIVDY-V-kVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "U4I-fc5kCKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, Response\n",
        "import json\n",
        "import subprocess\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "from urllib.parse import urlencode"
      ],
      "metadata": {
        "id": "xJpGDX7t6LA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update .env file Instructions\n",
        "\n",
        "After the cell below has started running, copy the public url provided by ngrok and update OLLAMA_BASE_URL in your `.env` file. Below is an example output that you will see.\n",
        "\n",
        "```\n",
        "NgrokTunnel: \"http://f9a8-34-73-238-198.ngrok.io\" -> \"http://localhost:5000\"\n",
        " * Serving Flask app '__main__'\n",
        " * Debug mode: off\n",
        "INFO:werkzeug:WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
        " * Running on http://127.0.0.1:5000\n",
        "INFO:werkzeug:Press CTRL+C to quit\n",
        "```\n",
        "\n",
        "DO NOT use this url, use the URL provided by the actual output from running the cell below. In this example, you will update your OLLAMA_BASE_URL variable with:\n",
        "\n",
        "```\n",
        "OLLAMA_BASE_URL=http://f9a8-34-73-238-198.ngrok.io\n",
        "```\n",
        "\n",
        "This url will change every time you rerun this cell, so remember to update your `.env` file when that happens."
      ],
      "metadata": {
        "id": "GZiFjH5QZFrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/api/generate', methods=['POST']) # Create route for generate_completion function\n",
        "def generate_completion():\n",
        "  model = request.form.get('model') # Get value model from the parameter url\n",
        "  prompt = request.form.get('prompt') # Get value prompt from the parameter url\n",
        "  persona = request.form.get('persona') # Get value persona from the parameter url\n",
        "  temperature = request.form.get('temperature') # Get value persona from the parameter url\n",
        "  json_data = { # JSON data to be sent in the POST request\n",
        "    \"model\": model or 'zephyr',\n",
        "    \"prompt\": prompt,\n",
        "    \"system\": persona or \"You are 2B from NieR Automata. Answer as 2B, the assistant, only.\",\n",
        "    \"options\" : { \"temperature\": temperature or 0.8 },\n",
        "    \"stream\": False\n",
        "  }\n",
        "  headers = { \"Content-Type\": \"application/json\" } # Set the headers for the request\n",
        "  response = requests.post(f'{OLLAMA_URL}/api/generate', json=json_data, headers=headers)\n",
        "  return json.loads(response.text)\n",
        "\n",
        "@app.route('/api/pull', methods=['POST'])\n",
        "def pull_model():\n",
        "  model_name = request.form.get('name')\n",
        "  json_data = { \"name\": model_name, \"stream\": False }\n",
        "  headers = { \"Content-Type\": \"application/json\" } # Set the headers for the request\n",
        "  response = requests.post(f'{OLLAMA_URL}/api/pull', json=json_data, headers=headers)\n",
        "  return json.loads(response.text)\n",
        "\n",
        "def main():\n",
        "  sub = subprocess.Popen(\"ollama serve\", shell=True, stdout=subprocess.PIPE)\n",
        "  http_tunnel = ngrok.connect(5000) # Open tunnel\n",
        "  print(http_tunnel)\n",
        "  app.run() # Run app\n",
        "\n",
        "if __name__ == '__main__': main()"
      ],
      "metadata": {
        "id": "wx4M5serF7NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leave the above cell running and the tab open\n",
        "\n",
        "This is to ensure the runtime does not disconnect and shut down the server.\n",
        "\n",
        "When you're done remember to disconnect the runtime."
      ],
      "metadata": {
        "id": "qDxifmXfZmGV"
      }
    }
  ]
}